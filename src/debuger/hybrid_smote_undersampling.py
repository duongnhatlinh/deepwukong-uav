#!/usr/bin/env python3
"""
Moderate Hybrid Dataset Balancer
- Limited SMOTE oversampling for vulnerable samples (max 2-3x original)
- Intelligent undersampling for safe samples
- Achieves target ratio without excessive synthetic data
"""
import pickle
import numpy as np
import networkx as nx
from pathlib import Path
from sklearn.neighbors import NearestNeighbors
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.metrics import pairwise_distances_argmin_min
from collections import defaultdict
import random
import json
from typing import List, Dict, Any
import sys
sys.path.append('.')


class ModerateGraphSMOTE:
    """Limited SMOTE for graph data - generates at most 2-3x original data"""
    
    def __init__(self, k_neighbors=5, max_multiplier=2.5):
        self.k_neighbors = k_neighbors
        self.max_multiplier = max_multiplier  # Max 2.5x original vulnerable samples

    def validate_and_fix_edges(self, graph):
        """Validate and fix missing 'c/d' attributes in edges"""
        fixed_count = 0
        
        for u, v, data in graph.edges(data=True):
            if 'c/d' not in data:
                data['c/d'] = 'd' if np.random.random() < 0.6 else 'c'
                fixed_count += 1
        
        return graph
        
    def extract_graph_features(self, xfg):
        """Extract numerical features from graph for SMOTE"""
        features = []
        
        # Graph-level features
        features.extend([
            len(xfg.nodes()),
            len(xfg.edges()),
            nx.density(xfg),
            len(list(nx.connected_components(xfg.to_undirected()))),
            nx.average_clustering(xfg.to_undirected()) if len(xfg.nodes()) > 0 else 0,
        ])
        
        # Node degree statistics
        degrees = [xfg.degree(n) for n in xfg.nodes()]
        if degrees:
            features.extend([
                np.mean(degrees),
                np.std(degrees) if len(degrees) > 1 else 0,
                max(degrees),
                min(degrees),
            ])
        else:
            features.extend([0, 0, 0, 0])
        
        # Token-level features
        all_tokens = []
        token_lengths = []
        
        for node in xfg.nodes(data=True):
            if 'code_sym_token' in node[1]:
                tokens = node[1]['code_sym_token']
                all_tokens.extend(tokens)
                token_lengths.append(len(tokens))
        
        features.extend([
            len(set(all_tokens)),
            np.mean(token_lengths) if token_lengths else 0,
            len(all_tokens),
        ])
        
        # Control/Data flow features
        control_edges = sum(1 for u, v, d in xfg.edges(data=True) 
                           if d.get('c/d') == 'c')
        data_edges = sum(1 for u, v, d in xfg.edges(data=True) 
                        if d.get('c/d') == 'd')
        
        features.extend([
            control_edges,
            data_edges,
            control_edges / len(xfg.edges()) if len(xfg.edges()) > 0 else 0,
        ])
        
        return np.array(features)
    
    def generate_moderate_synthetic_samples(self, minority_samples, target_minority_total):
        """Generate limited synthetic samples - at most max_multiplier x original"""
        
        original_count = len(minority_samples)
        max_allowed_total = int(original_count * self.max_multiplier)
        
        # Ensure we don't exceed the maximum multiplier
        actual_target = min(target_minority_total, max_allowed_total)
        n_synthetic = max(0, actual_target - original_count)
        
        print(f"ğŸ”„ Moderate SMOTE generation:")
        print(f"  ğŸ“Š Original vulnerable: {original_count:,}")
        print(f"  ğŸ¯ Target total: {target_minority_total:,}")
        print(f"  âš ï¸  Max allowed (â‰¤{self.max_multiplier}x): {max_allowed_total:,}")
        print(f"  âœ… Actual target: {actual_target:,}")
        print(f"  ğŸ†• Synthetic to generate: {n_synthetic:,}")
        
        if n_synthetic <= 0:
            print("  â„¹ï¸  No synthetic samples needed")
            return []
        
        # Check if we have enough samples for SMOTE
        if original_count < self.k_neighbors:
            print(f"  âš ï¸  Too few samples ({original_count}) for k-NN (k={self.k_neighbors})")
            print(f"  ğŸ”„ Using simple augmentation instead...")
            return self._simple_augmentation(minority_samples, n_synthetic)
        
        # Extract features for SMOTE
        print(f"  ğŸ” Extracting features for SMOTE...")
        features = []
        for sample in minority_samples:
            feat = self.extract_graph_features(sample['xfg'])
            features.append(feat)
        
        features = np.array(features)
        
        # Fit k-NN
        k = min(self.k_neighbors, len(features) - 1)
        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(features)
        
        synthetic_samples = []
        
        print(f"  ğŸ¨ Generating {n_synthetic:,} synthetic samples...")
        for i in range(n_synthetic):
            if i % 1000 == 0 and i > 0:
                print(f"    Progress: {i:,}/{n_synthetic:,}")
                
            # Random sample from minority class
            sample_idx = random.randint(0, len(minority_samples) - 1)
            base_sample = minority_samples[sample_idx]
            
            # Find its k-NN
            distances, indices = nbrs.kneighbors([features[sample_idx]])
            neighbor_indices = indices[0][1:]  # Exclude self
            
            if len(neighbor_indices) == 0:
                synthetic_xfg = self._simple_augment_graph(base_sample['xfg'], i)
            else:
                # Choose random neighbor
                neighbor_idx = random.choice(neighbor_indices)
                neighbor_sample = minority_samples[neighbor_idx]
                
                # Create synthetic sample
                synthetic_xfg = self._interpolate_graphs(
                    base_sample['xfg'], 
                    neighbor_sample['xfg'], 
                    alpha=random.random(),
                    seed=i
                )
            
            if synthetic_xfg is not None:
                synthetic_xfg = self.validate_and_fix_edges(synthetic_xfg)

                synthetic_sample = {
                    'path': f"moderate_smote_{i:05d}.xfg.pkl",
                    'xfg': synthetic_xfg,
                    'label': 1,
                    'synthetic': True,
                    'method': 'moderate_smote'
                }
                synthetic_samples.append(synthetic_sample)
        
        print(f"  âœ… Generated {len(synthetic_samples):,} synthetic vulnerable samples")
        print(f"  ğŸ“ˆ Total vulnerable: {original_count:,} + {len(synthetic_samples):,} = {original_count + len(synthetic_samples):,}")
        print(f"  ğŸ“Š Multiplier: {(original_count + len(synthetic_samples))/original_count:.1f}x")
        
        return synthetic_samples
    
    def _simple_augmentation(self, minority_samples, n_synthetic):
        """Simple augmentation when not enough samples for k-NN"""
        synthetic_samples = []
        
        for i in range(n_synthetic):
            # Pick random base sample
            base_sample = random.choice(minority_samples)
            synthetic_xfg = self._simple_augment_graph(base_sample['xfg'], i)
            
            synthetic_sample = {
                'path': f"simple_aug_{i:05d}.xfg.pkl",
                'xfg': synthetic_xfg,
                'label': 1,
                'synthetic': True,
                'method': 'simple_augmentation'
            }
            synthetic_samples.append(synthetic_sample)
        
        return synthetic_samples
    
    def _interpolate_graphs(self, graph1, graph2, alpha=0.5, seed=42):
        """Create synthetic graph by interpolating between two graphs"""
        np.random.seed(seed)
        
        # Choose the base graph (larger one for stability)
        if len(graph1.nodes()) >= len(graph2.nodes()):
            base_graph = graph1.copy()
            other_graph = graph2
        else:
            base_graph = graph2.copy()
            other_graph = graph1
        
        # Node interpolation: blend token features
        base_nodes = list(base_graph.nodes())
        other_nodes = list(other_graph.nodes())
        
        for i, node in enumerate(base_nodes):
            if 'code_sym_token' in base_graph.nodes[node]:
                base_tokens = base_graph.nodes[node]['code_sym_token'].copy()
                
                # Find corresponding node in other graph
                if i < len(other_nodes) and 'code_sym_token' in other_graph.nodes[other_nodes[i]]:
                    other_tokens = other_graph.nodes[other_nodes[i]]['code_sym_token']
                    
                    # Conservative token blending (less aggressive)
                    if np.random.random() < alpha * 0.3:  # Reduced blending probability
                        if len(other_tokens) > 0:
                            blend_ratio = np.random.uniform(0.1, 0.3)  # Less blending
                            n_from_other = int(len(base_tokens) * blend_ratio)
                            n_from_other = min(n_from_other, len(other_tokens))
                            
                            if n_from_other > 0:
                                selected_other = random.sample(other_tokens, n_from_other)
                                # Replace some base tokens
                                n_replace = min(n_from_other, len(base_tokens))
                                for j in range(n_replace):
                                    if j < len(selected_other):
                                        base_tokens[j] = selected_other[j]
                
                base_graph.nodes[node]['code_sym_token'] = base_tokens
        
        # Conservative edge interpolation
        other_edges = list(other_graph.edges(data=True))
        
        # Add very few edges from other graph
        for edge in other_edges:
            u, v, data = edge
            if u < len(base_nodes) and v < len(base_nodes):
                base_u = base_nodes[u] if u < len(base_nodes) else base_nodes[0]
                base_v = base_nodes[v] if v < len(base_nodes) else base_nodes[-1]
                
                if not base_graph.has_edge(base_u, base_v) and np.random.random() < 0.05:  # Very low probability
                    edge_data = data.copy()
                    if 'c/d' not in edge_data:
                        edge_data['c/d'] = 'd' if np.random.random() < 0.6 else 'c'
                    base_graph.add_edge(base_u, base_v, **edge_data)
        
        # Remove very few edges
        current_edges = list(base_graph.edges())
        n_remove = int(len(current_edges) * 0.02)  # Remove only 2%
        if n_remove > 0 and len(current_edges) > n_remove:
            edges_to_remove = random.sample(current_edges, n_remove)
            base_graph.remove_edges_from(edges_to_remove)
        
        # Ensure connectivity
        if len(base_graph.nodes()) > 1 and not nx.is_weakly_connected(base_graph):
            components = list(nx.weakly_connected_components(base_graph))
            for i in range(len(components) - 1):
                node1 = list(components[i])[0]
                node2 = list(components[i + 1])[0]
                base_graph.add_edge(node1, node2, synthetic=True, **{'c/d': 'd'})
        
        # Update metadata
        base_graph.graph['label'] = 1
        base_graph.graph['synthetic'] = True
        base_graph.graph['method'] = 'moderate_smote'
        base_graph.graph['alpha'] = alpha
        
        return base_graph
    
    def _simple_augment_graph(self, graph, seed):
        """Simple graph augmentation"""
        np.random.seed(seed)
        aug_graph = graph.copy()
        
        # Conservative token shuffling
        for node in aug_graph.nodes():
            if 'code_sym_token' in aug_graph.nodes[node]:
                tokens = aug_graph.nodes[node]['code_sym_token'].copy()
                if len(tokens) > 2 and np.random.random() < 0.1:  # Low probability
                    # Swap only 2 adjacent tokens
                    idx = random.randint(0, len(tokens) - 2)
                    tokens[idx], tokens[idx + 1] = tokens[idx + 1], tokens[idx]
                    aug_graph.nodes[node]['code_sym_token'] = tokens
        
        aug_graph.graph['label'] = 1
        aug_graph.graph['synthetic'] = True
        aug_graph.graph['method'] = 'simple_augment'
        
        return aug_graph


class IntelligentUndersampler:
    """Intelligent undersampling that preserves diversity"""
    
    def __init__(self, strategy='cluster_centroids'):
        self.strategy = strategy
    
    def extract_graph_features(self, xfg):
        """Extract features for clustering (same as SMOTE)"""
        features = []
        
        # Graph-level features
        features.extend([
            len(xfg.nodes()),
            len(xfg.edges()),
            nx.density(xfg),
            len(list(nx.connected_components(xfg.to_undirected()))),
            nx.average_clustering(xfg.to_undirected()) if len(xfg.nodes()) > 0 else 0,
        ])
        
        # Node degree statistics
        degrees = [xfg.degree(n) for n in xfg.nodes()]
        if degrees:
            features.extend([
                np.mean(degrees),
                np.std(degrees) if len(degrees) > 1 else 0,
                max(degrees),
                min(degrees),
            ])
        else:
            features.extend([0, 0, 0, 0])
        
        # Token features
        all_tokens = []
        token_lengths = []
        
        for node in xfg.nodes(data=True):
            if 'code_sym_token' in node[1]:
                tokens = node[1]['code_sym_token']
                all_tokens.extend(tokens)
                token_lengths.append(len(tokens))
        
        features.extend([
            len(set(all_tokens)),
            np.mean(token_lengths) if token_lengths else 0,
            len(all_tokens),
        ])
        
        # Control/Data flow features
        control_edges = sum(1 for u, v, d in xfg.edges(data=True) 
                           if d.get('c/d') == 'c')
        data_edges = sum(1 for u, v, d in xfg.edges(data=True) 
                        if d.get('c/d') == 'd')
        
        features.extend([
            control_edges,
            data_edges,
            control_edges / len(xfg.edges()) if len(xfg.edges()) > 0 else 0,
        ])
        
        return np.array(features)
    
    def undersample_majority(self, majority_samples, target_count):
        """Undersample majority class while preserving diversity"""
        print(f"ğŸ”½ Undersampling {len(majority_samples):,} safe samples to {target_count:,}")
        
        if target_count >= len(majority_samples):
            print("  No undersampling needed")
            return majority_samples
        
        if self.strategy == 'random':
            return self._random_undersample(majority_samples, target_count)
        elif self.strategy == 'cluster_centroids':
            return self._cluster_centroids_undersample(majority_samples, target_count)
        else:
            return self._random_undersample(majority_samples, target_count)
    
    def _random_undersample(self, samples, target_count):
        """Simple random undersampling"""
        print("  ğŸ“‹ Using random undersampling")
        selected = random.sample(samples, target_count)
        print(f"    âœ… Selected {len(selected):,} samples randomly")
        return selected
    
    def _cluster_centroids_undersample(self, samples, target_count):
        """Cluster-based undersampling with two-stage approach for large datasets"""
        print("  ğŸ¯ Using cluster centroids undersampling")
        
        # Two-stage approach for large datasets
        if len(samples) > 30000:
            print(f"    ğŸ“Š Large dataset detected ({len(samples):,} samples)")
            return self._two_stage_cluster_undersample(samples, target_count)
        
        # Direct clustering for smaller datasets
        print(f"    ğŸ”„ Extracting features from {len(samples):,} samples...")
        features = []
        for i, sample in enumerate(samples):
            if i % 10000 == 0 and i > 0:
                print(f"      Progress: {i:,}/{len(samples):,}")
            feat = self.extract_graph_features(sample['xfg'])
            features.append(feat)
        
        features = np.array(features)
        print(f"    âœ… Feature extraction completed. Shape: {features.shape}")
        
        try:
            print(f"    ğŸ¯ Running K-means clustering ({target_count} clusters)...")
            kmeans = KMeans(n_clusters=target_count, random_state=42, n_init=5)
            kmeans.fit(features)
            
            # Find samples closest to centroids
            centroids = kmeans.cluster_centers_
            closest_indices, _ = pairwise_distances_argmin_min(centroids, features)
            
            selected_samples = [samples[idx] for idx in closest_indices]
            print(f"    âœ… Selected {len(selected_samples):,} cluster centroids")
            return selected_samples
            
        except Exception as e:
            print(f"    âŒ Clustering failed: {e}")
            print(f"    ğŸ”„ Falling back to random sampling...")
            return self._random_undersample(samples, target_count)
    
    def _two_stage_cluster_undersample(self, samples, target_count):
        """Two-stage clustering for very large datasets"""
        print(f"    ğŸ“ STAGE 1: Pre-sampling large dataset")
        
        # Stage 1: Pre-sample to manageable size
        stage1_size = min(max(target_count * 3, 20000), len(samples))
        print(f"      â€¢ {len(samples):,} â†’ {stage1_size:,} samples")
        
        stage1_samples = random.sample(samples, stage1_size)
        print(f"      âœ… Stage 1 completed")
        
        # Stage 2: Cluster the pre-sampled data
        print(f"    ğŸ“ STAGE 2: Clustering pre-sampled data")
        
        features = []
        for sample in stage1_samples:
            feat = self.extract_graph_features(sample['xfg'])
            features.append(feat)
        
        features = np.array(features)
        
        try:
            kmeans = KMeans(n_clusters=target_count, random_state=42, n_init=5)
            kmeans.fit(features)
            
            centroids = kmeans.cluster_centers_
            closest_indices, _ = pairwise_distances_argmin_min(centroids, features)
            
            selected_samples = [stage1_samples[idx] for idx in closest_indices]
            print(f"      âœ… Stage 2 completed: {len(selected_samples):,} samples selected")
            return selected_samples
            
        except Exception as e:
            print(f"      âŒ Stage 2 failed: {e}")
            return random.sample(stage1_samples, target_count)


class ModerateHybridBalancer:
    """Moderate hybrid balancer - limited SMOTE + undersampling"""
    
    def __init__(self, original_path, output_path, target_ratio=0.35, 
                 max_vuln_multiplier=2.5, undersample_strategy='cluster_centroids', 
                 k_neighbors=5):
        """
        Args:
            original_path: Path to original unbalanced dataset
            output_path: Path to save balanced dataset
            target_ratio: Target ratio for vulnerable class (0.35 = 35% vulnerable)
            max_vuln_multiplier: Maximum multiplier for vulnerable samples (2.5 = max 2.5x)
            undersample_strategy: 'random' or 'cluster_centroids'
            k_neighbors: Number of neighbors for SMOTE
        """
        self.original_path = Path(original_path)
        self.output_path = Path(output_path)
        self.target_ratio = target_ratio
        self.max_vuln_multiplier = max_vuln_multiplier
        
        self.smote = ModerateGraphSMOTE(k_neighbors=k_neighbors, 
                                       max_multiplier=max_vuln_multiplier)
        self.undersampler = IntelligentUndersampler(strategy=undersample_strategy)
        
    def load_original_samples(self):
        """Load original unbalanced data"""
        all_samples = []
        
        for split in ['train', 'val', 'test']:
            split_file = self.original_path / f"{split}.json"
            if not split_file.exists():
                print(f"âš ï¸  Warning: {split_file} not found, skipping")
                continue
                
            with open(split_file, 'r') as f:
                paths = json.load(f)
            
            print(f"ğŸ“‚ Loading {split} split...")
            for i, path in enumerate(paths):
                if i % 20000 == 0 and i > 0:
                    print(f"    Progress: {i:,}/{len(paths):,}")
                    
                try:
                    with open(path, 'rb') as f:
                        xfg = pickle.load(f)
                    
                    all_samples.append({
                        'path': path,
                        'xfg': xfg,
                        'label': xfg.graph.get('label', -1),
                        'original_split': split
                    })
                except Exception as e:
                    continue
        
        print(f"âœ… Loaded {len(all_samples):,} total samples")
        return all_samples
    
    def balance_dataset(self):
        """Moderate hybrid balancing function"""
        print("ğŸ”„ Starting moderate hybrid dataset balancing...")
        print(f"  Strategy: Limited SMOTE (â‰¤{self.max_vuln_multiplier}x) + Intelligent undersampling")
        print(f"  Target ratio: {self.target_ratio:.1%} vulnerable")
        
        # Load original data
        all_samples = self.load_original_samples()
        
        # Separate by class
        vulnerable_samples = [s for s in all_samples if s['label'] == 1]
        safe_samples = [s for s in all_samples if s['label'] == 0]
        
        print(f"\nğŸ“Š Original distribution:")
        print(f"  ğŸ”´ Vulnerable: {len(vulnerable_samples):,}")
        print(f"  ğŸŸ¢ Safe: {len(safe_samples):,}")
        print(f"  ğŸ“ˆ Current ratio: {len(vulnerable_samples)/(len(vulnerable_samples)+len(safe_samples)):.1%}")
        
        # Calculate target counts
        current_total = len(all_samples)
        
        # Start with a reasonable total size
        max_total = min(current_total, 60000)  # Cap at 60K to keep manageable
        
        target_vulnerable = int(max_total * self.target_ratio)
        target_safe = max_total - target_vulnerable
        
        # Apply vulnerable multiplier constraint
        max_vulnerable_allowed = int(len(vulnerable_samples) * self.max_vuln_multiplier)
        if target_vulnerable > max_vulnerable_allowed:
            target_vulnerable = max_vulnerable_allowed
            target_safe = int(target_vulnerable * (1 - self.target_ratio) / self.target_ratio)
            max_total = target_vulnerable + target_safe
        
        print(f"\nğŸ¯ Target distribution:")
        print(f"  ğŸ”´ Target vulnerable: {target_vulnerable:,} (max {self.max_vuln_multiplier}x = {max_vulnerable_allowed:,})")
        print(f"  ğŸŸ¢ Target safe: {target_safe:,}")
        print(f"  ğŸ“Š Target total: {max_total:,}")
        print(f"  ğŸ“‰ Reduction from original: {(1 - max_total/current_total)*100:.1f}%")
        
        # Moderate SMOTE for vulnerable samples
        n_synthetic_needed = max(0, target_vulnerable - len(vulnerable_samples))
        if n_synthetic_needed > 0:
            synthetic_samples = self.smote.generate_moderate_synthetic_samples(
                vulnerable_samples, target_vulnerable)
            balanced_vulnerable = vulnerable_samples + synthetic_samples
        else:
            balanced_vulnerable = vulnerable_samples
            print(f"  â„¹ï¸  No oversampling needed for vulnerable samples")
        
        # Undersample safe samples
        if target_safe < len(safe_samples):
            balanced_safe = self.undersampler.undersample_majority(safe_samples, target_safe)
        else:
            balanced_safe = safe_samples
            print(f"  â„¹ï¸  No undersampling needed for safe samples")
        
        # Combine and shuffle
        all_balanced = balanced_vulnerable + balanced_safe
        random.shuffle(all_balanced)
        
        print(f"\nâœ… Final balanced distribution:")
        print(f"  ğŸ”´ Vulnerable: {len(balanced_vulnerable):,} ({len(balanced_vulnerable)/len(all_balanced):.1%})")
        print(f"    â€¢ Original: {len(vulnerable_samples):,}")
        print(f"    â€¢ Synthetic: {len(balanced_vulnerable) - len(vulnerable_samples):,}")
        print(f"    â€¢ Multiplier: {len(balanced_vulnerable)/len(vulnerable_samples):.1f}x")
        print(f"  ğŸŸ¢ Safe: {len(balanced_safe):,} ({len(balanced_safe)/len(all_balanced):.1%})")
        print(f"  ğŸ“Š Total: {len(all_balanced):,}")
        print(f"  ğŸ¯ Achieved ratio: {len(balanced_vulnerable)/len(all_balanced):.1%}")
        
        # Create new splits
        return self.create_splits(all_balanced)
    
    def create_splits(self, balanced_samples):
        """Create train/val/test splits"""
        total = len(balanced_samples)
        train_size = int(total * 0.8)
        val_size = int(total * 0.1)
        
        train = balanced_samples[:train_size]
        val = balanced_samples[train_size:train_size + val_size]
        test = balanced_samples[train_size + val_size:]
        
        return self.save_splits(train, val, test)
    
    def save_splits(self, train, val, test):
        """Save balanced splits"""
        self.output_path.mkdir(parents=True, exist_ok=True)
        xfg_dir = self.output_path / "XFG"
        xfg_dir.mkdir(exist_ok=True)
        
        split_paths = {}
        
        for split_name, samples in [('train', train), ('val', val), ('test', test)]:
            paths = []
            fixed_count = 0
            
            print(f"\nğŸ’¾ Saving {split_name} split ({len(samples):,} samples)...")
            
            for i, sample in enumerate(samples):
                if i % 5000 == 0 and i > 0:
                    print(f"    Progress: {i:,}/{len(samples):,}")
                
                # Validate graph
                graph = sample['xfg']
                
                # Fix missing 'c/d' attributes
                for u, v, data in graph.edges(data=True):
                    if 'c/d' not in data:
                        data['c/d'] = 'd' if np.random.random() < 0.6 else 'c'
                        fixed_count += 1
                
                label_str = "vuln" if sample['label'] == 1 else "safe"
                method_str = sample.get('method', 'original')
                filename = f"{split_name}_{label_str}_{method_str}_{i:05d}.xfg.pkl"
                path = xfg_dir / filename
                
                with open(path, 'wb') as f:
                    pickle.dump(sample['xfg'], f, pickle.HIGHEST_PROTOCOL)
                
                paths.append(str(path))
            
            if fixed_count > 0:
                print(f"    âœ… Fixed {fixed_count} edge attributes")
            
            split_paths[split_name] = paths
            
            # Save split file
            with open(self.output_path / f"{split_name}.json", 'w') as f:
                json.dump(paths, f, indent=2)
            
            # Print split statistics
            vuln_count = sum(1 for s in samples if s['label'] == 1)
            safe_count = len(samples) - vuln_count
            original_vuln = sum(1 for s in samples if s['label'] == 1 and not s.get('synthetic', False))
            synthetic_vuln = vuln_count - original_vuln
            
            print(f"    ğŸ“Š {split_name} statistics:")
            print(f"      â€¢ Total: {len(samples):,}")
            print(f"      â€¢ Vulnerable: {vuln_count:,} ({vuln_count/len(samples):.1%})")
            print(f"        - Original: {original_vuln:,}")
            print(f"        - Synthetic: {synthetic_vuln:,}")
            print(f"      â€¢ Safe: {safe_count:,} ({safe_count/len(samples):.1%})")
        
        print(f"\nâœ… Moderate hybrid balanced dataset saved to {self.output_path}")
        return self.output_path


def main():
    print("ğŸ”§ Running Moderate Hybrid Dataset Balancer...")
    print("ğŸ“‹ Strategy: Limited SMOTE (â‰¤2.5x) + Intelligent undersampling")
    
    balancer = ModerateHybridBalancer(
        original_path="data/UAV",
        output_path="data/UAV/moderate_balanced",
        target_ratio=0.35,  # 35% vulnerable, 65% safe
        max_vuln_multiplier=1.5,  # Max 2.5x vulnerable samples
        undersample_strategy='random',  # 'random' or 'cluster_centroids'
        k_neighbors=5
    )
    
    balanced_path = balancer.balance_dataset()
    
    print(f"\nâœ… Moderate hybrid balanced dataset created at: {balanced_path}")
    print("\nğŸ¯ Benefits of moderate approach:")
    print("  â€¢ Limited synthetic generation (â‰¤2.5x) prevents data dilution")
    print("  â€¢ Combines best of both worlds: oversampling + undersampling")
    print("  â€¢ Intelligent diversity-preserving undersampling")
    print("  â€¢ Conservative token blending in SMOTE")
    print("  â€¢ Manageable dataset size for efficient training")
    print("  â€¢ Realistic synthetic samples")
    
    print(f"\nğŸ“Š Expected results:")
    print(f"  â€¢ Original: 10,382 vuln (8.7%) + 109,101 safe (91.3%) = 119,483 total")
    print(f"  â€¢ Balanced: ~25,955 vuln (35%) + ~48,490 safe (65%) = ~74,445 total")
    print(f"  â€¢ Vulnerable breakdown: 10,382 original + ~15,573 synthetic")
    print(f"  â€¢ Safe reduction: 109,101 â†’ ~48,490 (56% reduction)")
    print(f"  â€¢ Total reduction: ~38% smaller dataset")
    print(f"  â€¢ Processing time: ~5-8 minutes")
    
    print(f"\nğŸ“ Update your config:")
    print(f"dataset.name: UAV/moderate_balanced")
    
    print(f"\nâš™ï¸  Customization options:")
    print(f"  â€¢ target_ratio: 0.3-0.4 (30-40% vulnerable)")
    print(f"  â€¢ max_vuln_multiplier: 2.0-3.0 (max synthetic multiplier)")
    print(f"  â€¢ undersample_strategy: 'cluster_centroids' or 'random'")


if __name__ == "__main__":
    main()
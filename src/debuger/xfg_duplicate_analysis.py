#!/usr/bin/env python3
"""
XFG Duplicate Analysis Tool for DeepWukong
Ph√¢n t√≠ch s·ªë l∆∞·ª£ng XFG tr√πng l·∫∑p, xung ƒë·ªôt v√† th·ªëng k√™ chi ti·∫øt
"""

import os
import hashlib
import networkx as nx
from typing import Dict, List, Tuple
from collections import defaultdict
from tqdm import tqdm
from argparse import ArgumentParser
from omegaconf import OmegaConf, DictConfig
from os.path import join, exists
import pickle
import random
import json


def read_gpickle(filename):
    try:
        with open(filename, 'rb') as f:
            graph = pickle.load(f)
        return graph
    except Exception as e:
        print(f"Error reading gpickle file: {e}")

def getMD5(s: str) -> str:
    """T·∫°o MD5 hash cho string"""
    hl = hashlib.md5()
    hl.update(s.encode("utf-8"))
    return hl.hexdigest()


def analyze_xfg_duplicates(xfg_paths: List[str], verbose: bool = True) -> Dict:
    """
    Ph√¢n t√≠ch chi ti·∫øt c√°c XFG tr√πng l·∫∑p v√† xung ƒë·ªôt
    
    Args:
        xfg_paths: List ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c file XFG
        verbose: In chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω
        
    Returns:
        Dictionary ch·ª©a th·ªëng k√™ chi ti·∫øt
    """
    
    # Dictionary ƒë·ªÉ l∆∞u th√¥ng tin XFG theo MD5
    md5_to_xfgs = defaultdict(list)
    
    # Counters
    total_xfgs = len(xfg_paths)
    processed = 0
    
    # Th√™m counters cho label=1 v√† label=0 ban ƒë·∫ßu
    original_label1 = 0
    original_label0 = 0

    print(f"üîç Analyzing {total_xfgs} XFGs...")
    
    # Process each XFG
    for xfg_path in tqdm(xfg_paths, desc="Processing XFGs"):
        try:
            xfg = read_gpickle(xfg_path)
            label = xfg.graph["label"]
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng label=1 v√† label=0 ban ƒë·∫ßu
            if label == 1:
                original_label1 += 1
            elif label == 0:
                original_label0 += 1
            
            # T·∫°o MD5 cho m·ªói node d·ª±a tr√™n symbolic token
            for ln in xfg:
                if "code_sym_token" in xfg.nodes[ln]:
                    ln_md5 = getMD5(str(xfg.nodes[ln]["code_sym_token"]))
                    xfg.nodes[ln]["md5"] = ln_md5
                else:
                    # Fallback n·∫øu ch∆∞a c√≥ symbolic token
                    ln_md5 = getMD5(str(ln))
                    xfg.nodes[ln]["md5"] = ln_md5
            
            # T·∫°o MD5 cho to√†n b·ªô XFG d·ª±a tr√™n c·∫•u tr√∫c edges
            edges_md5 = []
            for edge in xfg.edges:
                edge_md5 = xfg.nodes[edge[0]]["md5"] + "_" + xfg.nodes[edge[1]]["md5"]
                edges_md5.append(edge_md5)
            
            # T·∫°o unique ID cho XFG
            xfg_md5 = getMD5(str(sorted(edges_md5)))
            
            # L∆∞u th√¥ng tin XFG
            xfg_info = {
                'path': xfg_path,
                'label': label,
                'num_nodes': len(xfg.nodes),
                'num_edges': len(xfg.edges),
                'file_path': xfg.graph.get("file_paths", ["unknown"])[0],
                'key_line': xfg.graph.get("key_line", -1)
            }
            
            md5_to_xfgs[xfg_md5].append(xfg_info)
            processed += 1
            
        except Exception as e:
            if verbose:
                print(f"‚ö†Ô∏è  Error processing {xfg_path}: {e}")
            continue
    
    print(f"‚úÖ Successfully processed {processed}/{total_xfgs} XFGs")
    
    # Analyze duplicates and conflicts
    stats = analyze_duplicate_statistics(md5_to_xfgs, verbose)
    
    # Th√™m s·ªë l∆∞·ª£ng label=1 v√† label=0 ban ƒë·∫ßu v√†o stats
    stats['original_label1'] = original_label1
    stats['original_label0'] = original_label0
    
    return stats


def analyze_duplicate_statistics(md5_to_xfgs: Dict, verbose: bool = True) -> Dict:
    """
    Ph√¢n t√≠ch th·ªëng k√™ chi ti·∫øt v·ªÅ duplicates v√† conflicts
    """
    
    # Counters
    unique_xfgs = 0
    duplicate_groups = 0
    conflict_groups = 0
    
    vul_duplicates = 0  # XFG vulnerable tr√πng l·∫∑p
    safe_duplicates = 0  # XFG safe tr√πng l·∫∑p
    conflicts = 0  # XFG xung ƒë·ªôt
    
    # Chi ti·∫øt v·ªÅ duplicates
    vul_duplicate_details = []
    safe_duplicate_details = []
    conflict_details = []
    
    # Th√™m counters cho label=1 v√† label=0 sau khi deduplicate
    dedup_label1 = 0
    dedup_label0 = 0
    
    # Th√™m counters cho unique label=1 v√† label=0
    unique_label1 = 0
    unique_label0 = 0
    
    # Th√™m list ƒë·ªÉ l∆∞u tr·ªØ paths cho vi·ªác t·∫°o dataset c√¢n b·∫±ng
    unique_label1_paths = []
    unique_label0_paths = []
    dedup_vul_details = [] # list of {'path': path, 'count': count}
    dedup_safe_details = [] # list of {'path': path, 'count': count}
    
    print("\nüìä ANALYZING DUPLICATE STATISTICS...")
    print("=" * 60)
    
    for xfg_md5, xfg_list in md5_to_xfgs.items():
        if len(xfg_list) == 1:
            # Unique XFG
            unique_xfgs += 1
            # ƒê·∫øm label cho unique XFG
            label = xfg_list[0]['label']
            path = xfg_list[0]['path']
            if label == 1:
                dedup_label1 += 1
                unique_label1 += 1
                unique_label1_paths.append(path)
            elif label == 0:
                dedup_label0 += 1
                unique_label0 += 1
                unique_label0_paths.append(path)
        else:
            # Multiple XFGs with same structure
            labels = set([xfg['label'] for xfg in xfg_list])
            
            if len(labels) == 1:
                # Duplicates (same structure, same label)
                duplicate_groups += 1
                label = list(labels)[0]
                # Ch·ªâ gi·ªØ l·∫°i 1 b·∫£n ƒë·∫°i di·ªán cho dedup
                path = xfg_list[0]['path']
                count = len(xfg_list)
                if label == 1:
                    dedup_label1 += 1
                    dedup_vul_details.append({'path': path, 'count': count})
                elif label == 0:
                    dedup_label0 += 1
                    dedup_safe_details.append({'path': path, 'count': count})
                
                if label == 1:  # Vulnerable
                    vul_duplicates += len(xfg_list) - 1  # Tr·ª´ 1 v√¨ ch·ªâ ƒë·∫øm b·∫£n sao
                    vul_duplicate_details.append({
                        'md5': xfg_md5,
                        'count': len(xfg_list),
                        'xfgs': xfg_list
                    })
                elif label == 0:  # Safe
                    safe_duplicates += len(xfg_list) - 1
                    safe_duplicate_details.append({
                        'md5': xfg_md5,
                        'count': len(xfg_list),
                        'xfgs': xfg_list
                    })
                
                if verbose:
                    label_str = "VUL" if label == 1 else "SAFE"
                    print(f"üîÑ DUPLICATE [{label_str}]: {len(xfg_list)} XFGs with same structure")
                    for i, xfg in enumerate(xfg_list[:3]):  # Show first 3
                        print(f"   {i+1}. {xfg['path']} (nodes: {xfg['num_nodes']}, edges: {xfg['num_edges']})")
                    if len(xfg_list) > 3:
                        print(f"   ... and {len(xfg_list) - 3} more")
                    print()
            else:
                # Conflicts (same structure, different labels)
                conflict_groups += 1
                conflicts += len(xfg_list)
                conflict_details.append({
                    'md5': xfg_md5,
                    'count': len(xfg_list),
                    'labels': list(labels),
                    'xfgs': xfg_list
                })
                # Kh√¥ng c·ªông v√†o dedup v√¨ xung ƒë·ªôt kh√¥ng gi·ªØ l·∫°i
                if verbose:
                    print(f"‚ö†Ô∏è  CONFLICT: {len(xfg_list)} XFGs with same structure but different labels {list(labels)}")
                    for i, xfg in enumerate(xfg_list):
                        label_str = "VUL" if xfg['label'] == 1 else "SAFE"
                        print(f"   {i+1}. [{label_str}] {xfg['path']} (nodes: {xfg['num_nodes']}, edges: {xfg['num_edges']})")
                    print()
    # T·∫°o summary statistics
    total_original_xfgs = sum(len(xfg_list) for xfg_list in md5_to_xfgs.values())
    
    # Th·ªëng k√™ s·ªë l∆∞·ª£ng nh√≥m duplicate theo s·ªë l∆∞·ª£ng file duplicate
    from collections import Counter
    # T√°ch th·ªëng k√™ cho label=1 v√† label=0
    vul_duplicate_group_size_stats = Counter()
    for group in vul_duplicate_details:
        sz = group['count']
        vul_duplicate_group_size_stats[sz] += 1
    vul_duplicate_group_size_stats = dict(sorted(vul_duplicate_group_size_stats.items()))
    
    safe_duplicate_group_size_stats = Counter()
    for group in safe_duplicate_details:
        sz = group['count']
        safe_duplicate_group_size_stats[sz] += 1
    safe_duplicate_group_size_stats = dict(sorted(safe_duplicate_group_size_stats.items()))

    stats = {
        'total_original_xfgs': total_original_xfgs,
        'unique_structures': len(md5_to_xfgs),
        'unique_xfgs': unique_xfgs,
        'unique_label1': unique_label1,
        'unique_label0': unique_label0,
        'duplicate_groups': duplicate_groups,
        'conflict_groups': conflict_groups,
        
        'vul_duplicates': vul_duplicates,
        'safe_duplicates': safe_duplicates,
        'total_duplicates': vul_duplicates + safe_duplicates,
        'conflicts': conflicts,
        
        'vul_duplicate_details': vul_duplicate_details,
        'safe_duplicate_details': safe_duplicate_details,
        'conflict_details': conflict_details,
        
        'final_dataset_size': dedup_label1 + dedup_label0,  # S·ªë l∆∞·ª£ng XFG sau khi lo·∫°i duplicates v√† conflicts
        'dedup_label1': dedup_label1,
        'dedup_label0': dedup_label0,
        'vul_duplicate_group_size_stats': vul_duplicate_group_size_stats,
        'safe_duplicate_group_size_stats': safe_duplicate_group_size_stats,
        
        # Paths for balancing
        'unique_label1_paths': unique_label1_paths,
        'unique_label0_paths': unique_label0_paths,
        'dedup_vul_details': dedup_vul_details,
        'dedup_safe_details': dedup_safe_details,
    }
    
    return stats


def print_summary_report(stats: Dict):
    """In b√°o c√°o t·ªïng k·∫øt"""
    
    print("\n" + "="*80)
    print("üìã SUMMARY REPORT")
    print("="*80)
    
    print(f"üì¶ Total Original XFGs: {stats['total_original_xfgs']}")
    print(f"   ‚Ä¢ Original label=1: {stats.get('original_label1', '?')}")
    print(f"   ‚Ä¢ Original label=0: {stats.get('original_label0', '?')}")
    print(f"üîß Unique Structures: {stats['unique_structures']}")
    print(f"‚ú® Unique XFGs: {stats['unique_xfgs']}")
    print(f"   ‚Ä¢ Unique label=1: {stats.get('unique_label1', '?')}")
    print(f"   ‚Ä¢ Unique label=0: {stats.get('unique_label0', '?')}")
    print()
    
    print("üîÑ DUPLICATES:")
    print(f"   ‚Ä¢ Vulnerable (label=1) duplicates: {stats['vul_duplicates']} XFGs")
    print(f"   ‚Ä¢ Safe (label=0) duplicates: {stats['safe_duplicates']} XFGs") 
    print(f"   ‚Ä¢ Total duplicates: {stats['total_duplicates']} XFGs")
    print(f"   ‚Ä¢ Duplicate groups: {stats['duplicate_groups']} groups")
    print(f"   ‚Ä¢ VUL duplicate group size stats (group_size: num_groups): {stats.get('vul_duplicate_group_size_stats', {})}")
    print(f"   ‚Ä¢ SAFE duplicate group size stats (group_size: num_groups): {stats.get('safe_duplicate_group_size_stats', {})}")
    print()
    
    print("‚ö†Ô∏è  CONFLICTS:")
    print(f"   ‚Ä¢ XFGs with conflicts: {stats['conflicts']} XFGs")
    print(f"   ‚Ä¢ Conflict groups: {stats['conflict_groups']} groups")
    print()
    
    print("üìä FINAL DATASET:")
    print(f"   ‚Ä¢ After deduplication: {stats['final_dataset_size']} XFGs")
    print(f"   ‚Ä¢ Deduplicated label=1: {stats.get('dedup_label1', '?')}")
    print(f"   ‚Ä¢ Deduplicated label=0: {stats.get('dedup_label0', '?')}")
    print(f"   ‚Ä¢ Reduction rate: {((stats['total_original_xfgs'] - stats['final_dataset_size']) / stats['total_original_xfgs'] * 100):.1f}%")
    
    print("\n" + "="*80)


def create_balanced_dataset(stats: Dict, output_file: str):
    """
    T·∫°o b·ªô d·ªØ li·ªáu c√¢n b·∫±ng theo t·ªâ l·ªá 3:7 (vul:safe) v√† l∆∞u danh s√°ch file
    """
    print("‚öñÔ∏è  CREATING BALANCED DATASET...")
    
    # 1. L·∫•y to√†n b·ªô XFG label=1 sau deduplicate
    vul_paths = stats['unique_label1_paths'] + [item['path'] for item in stats['dedup_vul_details']]
    num_vul = len(vul_paths)
    
    # 2. T√≠nh to√°n s·ªë l∆∞·ª£ng XFG label=0 c·∫ßn thi·∫øt
    target_num_safe = int(num_vul / 3 * 7)
    
    print(f"   ‚Ä¢ Target Vul (label=1): {num_vul}")
    print(f"   ‚Ä¢ Target Safe (label=0): {target_num_safe}")
    
    # 3. L·∫•y XFG label=0 t·ª´ c√°c ngu·ªìn ∆∞u ti√™n
    selected_safe_paths = []
    
    # ∆Øu ti√™n 1: L·∫•y t·ª´ c√°c nh√≥m duplicate l·ªõn (size >= 10)
    safe_from_large_groups = [
        item['path'] for item in stats['dedup_safe_details'] 
        if item['count'] >= 10
    ]
    selected_safe_paths.extend(safe_from_large_groups)
    
    print(f"   ‚Ä¢ Selected {len(selected_safe_paths)} samples from large duplicate groups (size >= 10)")
    
    # ∆Øu ti√™n 2: L·∫•y ng·∫´u nhi√™n t·ª´ c√°c XFG unique label=0
    needed = target_num_safe - len(selected_safe_paths)
    if needed > 0:
        unique_safe_paths = stats['unique_label0_paths']
        if needed >= len(unique_safe_paths):
            # N·∫øu c·∫ßn nhi·ªÅu h∆°n s·ªë unique hi·ªán c√≥, l·∫•y h·∫øt
            selected_safe_paths.extend(unique_safe_paths)
            print(f"   ‚Ä¢ Selected all {len(unique_safe_paths)} unique samples")
        else:
            # N·∫øu kh√¥ng, l·∫•y ng·∫´u nhi√™n
            random_unique_paths = random.sample(unique_safe_paths, needed)
            selected_safe_paths.extend(random_unique_paths)
            print(f"   ‚Ä¢ Randomly selected {len(random_unique_paths)} unique samples")
    
    final_num_safe = len(selected_safe_paths)

    # 4. K·∫øt h·ª£p v√† l∆∞u file
    final_paths = vul_paths + selected_safe_paths
    random.shuffle(final_paths) # X√°o tr·ªôn b·ªô d·ªØ li·ªáu cu·ªëi c√πng
    
    report = {
        'description': 'Balanced dataset with Vul:Safe ratio ~3:7',
        'num_vulnerable': num_vul,
        'num_safe': final_num_safe,
        'total': len(final_paths),
        'paths': final_paths
    }
    
    with open(output_file, 'w') as f:
        json.dump(report, f, indent=2)
        
    print("\n" + "-"*60)
    print(f"‚úÖ Balanced dataset created successfully!")
    print(f"   ‚Ä¢ Final Vul (label=1): {num_vul}")
    print(f"   ‚Ä¢ Final Safe (label=0): {final_num_safe}")
    print(f"   ‚Ä¢ Total XFGs: {len(final_paths)}")
    print(f"   ‚Ä¢ Saved to: {output_file}")
    print("-" * 60)


def get_all_xfg_paths(data_folder: str, dataset_name: str) -> List[str]:
    """
    L·∫•y t·∫•t c·∫£ ƒë∆∞·ªùng d·∫´n XFG t·ª´ th∆∞ m·ª•c d·ªØ li·ªáu
    """
    XFG_root_path = join(data_folder, dataset_name, "XFG")
    
    if not exists(XFG_root_path):
        raise FileNotFoundError(f"XFG directory not found: {XFG_root_path}")
    
    xfg_paths = []
    testcaseids = os.listdir(XFG_root_path)
    
    for testcase in testcaseids:
        testcase_root_path = join(XFG_root_path, testcase)
        if not os.path.isdir(testcase_root_path):
            continue
            
        for k in ["arith", "array", "call", "ptr"]:
            k_root_path = join(testcase_root_path, k)
            if not exists(k_root_path):
                continue
                
            xfg_files = os.listdir(k_root_path)
            for xfg_file in xfg_files:
                if xfg_file.endswith('.xfg.pkl'):
                    xfg_path = join(k_root_path, xfg_file)
                    xfg_paths.append(xfg_path)
    
    return xfg_paths


def save_detailed_report(stats: Dict, output_file: str):
    """L∆∞u b√°o c√°o chi ti·∫øt ra file"""
    
    import json
    
    # Convert ƒë·ªÉ c√≥ th·ªÉ serialize JSON
    report = {
        'summary': {
            'total_original_xfgs': stats['total_original_xfgs'],
            'unique_structures': stats['unique_structures'],
            'unique_xfgs': stats['unique_xfgs'],
            'vul_duplicates': stats['vul_duplicates'],
            'safe_duplicates': stats['safe_duplicates'],
            'conflicts': stats['conflicts'],
            'final_dataset_size': stats['final_dataset_size']
        },
        'vul_duplicate_groups': len(stats['vul_duplicate_details']),
        'safe_duplicate_groups': len(stats['safe_duplicate_details']),
        'conflict_groups': len(stats['conflict_details'])
    }
    
    with open(output_file, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f"üíæ Detailed report saved to: {output_file}")


def main():
    parser = ArgumentParser(description="Analyze XFG duplicates and conflicts")
    parser.add_argument("-c", "--config", 
                       help="Path to YAML configuration file",
                       default="configs/dwk.yaml", type=str)
    parser.add_argument("-o", "--output",
                       help="Output file for detailed report",
                       default="xfg_analysis_report.json", type=str)
    parser.add_argument("-v", "--verbose",
                       help="Verbose output", action="store_true")
    parser.add_argument("--balance-output",
                        help="Output file for balanced dataset paths (JSON format)",
                        type=str, default=None)
    
    args = parser.parse_args()
    
    # Load config
    try:
        config = OmegaConf.load(args.config)
        data_folder = config.data_folder
        dataset_name = config.dataset.name
    except Exception as e:
        print(f"‚ùå Error loading config: {e}")
        return
    
    try:
        # Get all XFG paths
        print(f"üîç Scanning XFGs in {data_folder}/{dataset_name}/XFG/...")
        xfg_paths = get_all_xfg_paths(data_folder, dataset_name)
        print(f"üìÅ Found {len(xfg_paths)} XFG files")
        
        if len(xfg_paths) == 0:
            print("‚ùå No XFG files found!")
            return
        
        # Analyze duplicates
        stats = analyze_xfg_duplicates(xfg_paths, verbose=args.verbose)
        
        # Print summary
        print_summary_report(stats)
        
        # Save detailed report
        # save_detailed_report(stats, args.output)
        
        # Create balanced dataset if requested
        # if args.balance_output:
        #     create_balanced_dataset(stats, args.balance_output)
        
    except Exception as e:
        print(f"‚ùå Error during analysis: {e}")


if __name__ == "__main__":
    main()